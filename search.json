[
  {
    "objectID": "data/student-survey.html",
    "href": "data/student-survey.html",
    "title": "CNN vs. Onion",
    "section": "",
    "text": "The questions in this part of the homework/quiz relates to the CNN vs. Onion activity that you have completed.\nWhat aspects of this activity have impacted your learning positively?\nWhat aspects of this activity have impacted your learning negatively?\nWhat is your overall opinion of this activity?\nDo you have any recommendations on how this activity can be improved for future students?"
  },
  {
    "objectID": "mytemp_delete.html",
    "href": "mytemp_delete.html",
    "title": "Untitled",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "mytemp_delete.html#r-markdown",
    "href": "mytemp_delete.html#r-markdown",
    "title": "Untitled",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "mytemp_delete.html#including-plots",
    "href": "mytemp_delete.html#including-plots",
    "title": "Untitled",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "bayesian-activity/cnn-onion-beta-binomial.html",
    "href": "bayesian-activity/cnn-onion-beta-binomial.html",
    "title": "CNN vs. The Onion - Beta Binomial",
    "section": "",
    "text": "The goal of this activity is to explore how prior beliefs, what we think is probable before seeing any data, can influence the conclusions we draw after seeing new evidence: our posterior beliefs. To make things interesting, we will use a quiz where you’ll try to tell whether a headline came from CNN (a real news site) or The Onion (a fake, satirical news site).\nBefore taking the quiz, you’ll think about how many headlines you expect a person might guess correctly, and you’ll turn that guess into a prior model, or starting point for your beliefs. Then, you’ll update your beliefs using data from actual quiz results and arrive at a posterior model. You will compare three different prior models representing other beliefs: someone expecting a person to guess most answers correctly (optimistic), someone undecided (no prior knowledge) about whether a person will guess many right or wrong (undecided), and someone expecting to guess most answers incorrectly (pessimistic) about how many answers they will get correct in the game.\n\n\n\n\n\n\nTipLearning objectives\n\n\n\nBy the end of the activity you’ll be able to:\n\nConstruct prior and posterior models\nSee how models update when we get new information\nMake and interpret plots of different prior and posterior models\nCalculate and compare summary statistics like the mean, mode, and standard deviation of these models.\n\n\n\nLet’s dive in and see how well we can guess the news from fiction and learn about Bayesian thinking along the way!"
  },
  {
    "objectID": "bayesian-activity/cnn-onion-beta-binomial.html#activity-introduction",
    "href": "bayesian-activity/cnn-onion-beta-binomial.html#activity-introduction",
    "title": "CNN vs. The Onion - Beta Binomial",
    "section": "",
    "text": "The goal of this activity is to explore how prior beliefs, what we think is probable before seeing any data, can influence the conclusions we draw after seeing new evidence: our posterior beliefs. To make things interesting, we will use a quiz where you’ll try to tell whether a headline came from CNN (a real news site) or The Onion (a fake, satirical news site).\nBefore taking the quiz, you’ll think about how many headlines you expect a person might guess correctly, and you’ll turn that guess into a prior model, or starting point for your beliefs. Then, you’ll update your beliefs using data from actual quiz results and arrive at a posterior model. You will compare three different prior models representing other beliefs: someone expecting a person to guess most answers correctly (optimistic), someone undecided (no prior knowledge) about whether a person will guess many right or wrong (undecided), and someone expecting to guess most answers incorrectly (pessimistic) about how many answers they will get correct in the game.\n\n\n\n\n\n\nTipLearning objectives\n\n\n\nBy the end of the activity you’ll be able to:\n\nConstruct prior and posterior models\nSee how models update when we get new information\nMake and interpret plots of different prior and posterior models\nCalculate and compare summary statistics like the mean, mode, and standard deviation of these models.\n\n\n\nLet’s dive in and see how well we can guess the news from fiction and learn about Bayesian thinking along the way!"
  },
  {
    "objectID": "bayesian-activity/cnn-onion-beta-binomial.html#cnn-vs-the-onion",
    "href": "bayesian-activity/cnn-onion-beta-binomial.html#cnn-vs-the-onion",
    "title": "CNN vs. The Onion - Beta Binomial",
    "section": "CNN vs The Onion",
    "text": "CNN vs The Onion\nCNN (the Cable News Network) is widely considered a reputable news source. The Onion, on the other hand, is (according to Wikipedia) “an American news satire organization. It is an entertainment newspaper and a website featuring satirical articles reporting on international, national, and local news.” Another way of putting it - The Onion is “fake news” for entertainment purposes.\nIn this exercise you will assess people’s ability to determine real news stories published on cnn.com from fake news stories published on theonion.com.\n\n\n\nFrontpage of the Onion. Photo: Casey Bisson, via Flickr"
  },
  {
    "objectID": "bayesian-activity/cnn-onion-beta-binomial.html#packages",
    "href": "bayesian-activity/cnn-onion-beta-binomial.html#packages",
    "title": "CNN vs. The Onion - Beta Binomial",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(bayesrules)"
  },
  {
    "objectID": "bayesian-activity/cnn-onion-beta-binomial.html#priors",
    "href": "bayesian-activity/cnn-onion-beta-binomial.html#priors",
    "title": "CNN vs. The Onion - Beta Binomial",
    "section": "Priors",
    "text": "Priors\nThe CNN vs. The Onion quiz consists of 15 questions. Each question has the same possible answers: CNN or The Onion. Before we take the quiz, think about what proportion of questions you expect someone to guess correctly on the quiz. You might think about a person’s ability to determine fact from fiction or their familiarity with CNN and The Onion.\nLet \\(\\pi\\) be the proportion of correct answers a person guesses right in the CNN vs the Onion quiz. Keeping that number in mind, let’s explore in the table below, three different priors for \\(\\pi\\)\n\n\n\nOptimistic\nUndecided\nPessimistic\n\n\n\n\nBeta(8, 3)\nBeta(1, 1)\nBeta(5, 10)"
  },
  {
    "objectID": "bayesian-activity/cnn-onion-beta-binomial.html#why-do-we-use-the-beta-distribution",
    "href": "bayesian-activity/cnn-onion-beta-binomial.html#why-do-we-use-the-beta-distribution",
    "title": "CNN vs. The Onion - Beta Binomial",
    "section": "Why do we use the Beta distribution?",
    "text": "Why do we use the Beta distribution?\nThe Beta distribution is commonly used as a prior for proportions because it is defined on the interval \\([0, 1]\\), just like any probability.\nIts two shape parameters \\(\\alpha\\) and \\(\\beta\\) can be interpreted as representing prior pseudo-observations:\n\n\\(\\alpha - 1\\): prior successes\n\\(\\beta - 1\\): prior failures\n\nThese shape parameters not only determine the mean of the distribution (calculated as \\(\\alpha / (\\alpha + \\beta)\\) but also control its skewness and informativeness.\nDifferent values for \\(\\alpha\\) and \\(\\beta\\) lead to different shapes, skewness, and central tendencides of the Beta distribution. See if you can pick out some general patterns with the relationship between \\(\\alpha\\) and \\(\\beta\\) in the plot below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions\n\n\n\nWhat happens when:\n\n\\(\\alpha &gt; \\beta\\)\n\\(\\alpha &lt; \\beta\\)\n\\(\\alpha = \\beta\\)\n\nWhat happens as \\(\\alpha\\) and \\(\\beta\\) get larger and further apart?\n\n\n\n\n\n\n\n\nTipAnswers\n\n\n\n\n\nSkewness and Symmetry\n\nIf \\(\\alpha &lt; \\beta\\), the distribution is right-skewed, suggesting a belief that the proportion is likely to be low (e.g., Beta(1, 5)).\nIf \\(\\alpha &gt; \\beta\\), the distribution is left-skewed, placing more weight on higher proportions (e.g., Beta(5, 1)).\nIf \\(\\alpha = \\beta\\), the distribution is symmetric and centered at 0.5 (e.g., Beta(5, 5)).\n\nThe further apart \\(\\alpha\\) and \\(\\beta\\) are, the more skewed the distribution becomes. Symmetric models like Beta(20, 20) or Beta(5, 5) show increasingly narrow concentration around the mean as the total number of pseudo-observations \\(\\alpha + \\beta\\) increases.\n\n\n\n\nPlotting the Priors\n\n\n\n\n\n\n\n\n\n\n\nWhere does your prior fall?\nReturning to your own prior, replace my_alpha and my_beta with the \\(\\alpha\\) and \\(\\beta\\) from your prior distribution.\n\n\n\n\n\n\n\n\nLooking at the graph of your prior, which person is your prior most similar to: Optimistic, Undecided or Pessimistic?\n\n\n\n\n\n\nNoteVocabulary\n\n\n\nWe often describe priors in terms of how much information they give about the unknown variable. Priors are often described as:\n\nInformative prior: An informative prior reflects specific information about the unknown variable with high certainty (i.e. low variability).\nVague (diffuse) prior: A vague or diffuse prior reflects little specific information about the unknown variable. A flat prior, which assigns equal prior plausibility to all possible values of the variable, is a special case.\n\n\n\nLet’s take a look at a highly informative prior from someone who is very optimistic about individuals getting answers correct.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipReflection:\n\n\n\nReferring to your Reflection handout, write down your answers to the following questions:\n\nWhat do you notice about the very optimistic prior. Is it more informative? What do you notice about the plot?\nHow would you classify your prior? Is it informative or vague? Why?"
  },
  {
    "objectID": "bayesian-activity/cnn-onion-beta-binomial.html#updating-with-data",
    "href": "bayesian-activity/cnn-onion-beta-binomial.html#updating-with-data",
    "title": "CNN vs. The Onion - Beta Binomial",
    "section": "Updating with data",
    "text": "Updating with data\n\nBased on observed data, we will update our posterior understanding using the four prior models and our own prior model. We will do this with our own score, our neighbor’s scores, and the class or dataset score.\nNext, we calculate the summary statistics for the prior and posterior for all four priors using the function summarize_beta_binomial:\n\n\n\n\n\n\n\nNotesummarize_beta_binomial\n\n\n\n\nsummarize_beta_binomial(alpha, beta, y = NULL, n = NULL) function summarizes the mean, mode, and variance of the prior and posterior Beta models of \\(\\pi\\).\nArguments:\n\nalpha, beta: positive shape parameters of the prior Beta model\ny: number of successes\nn: number of trials\n\n\n\n\n\nNext, we plot the prior, likelihood, and the posterior for all four people.\nLastly, we examine the effect of different priors on the posterior."
  },
  {
    "objectID": "bayesian-activity/cnn-onion-beta-binomial.html#take-the-quiz",
    "href": "bayesian-activity/cnn-onion-beta-binomial.html#take-the-quiz",
    "title": "CNN vs. The Onion - Beta Binomial",
    "section": "Take the quiz",
    "text": "Take the quiz\nLet’s take the quiz and add our data to the dataset of trials and successes for a person taking this quiz.\nEach of you will take a quiz consisting of 15 questions. Each question has the same possible answers: CNN or The Onion. You can take the quiz through our google form:\nLoading…\nNote down how many you get correct. We will compare this with our neighbors and pool data from the class."
  },
  {
    "objectID": "bayesian-activity/cnn-onion-beta-binomial.html#adding-data",
    "href": "bayesian-activity/cnn-onion-beta-binomial.html#adding-data",
    "title": "CNN vs. The Onion - Beta Binomial",
    "section": "Adding data",
    "text": "Adding data\nChoose the priors that interest you and we will update them as follows below:\n\nAdd your quiz data\n\nLet’s update the prior model with some data. Add the number you got correct and enter your prior from before. Your n column should be 15 to reflect that you answered 15 questions.\n\nCompare with your classmates\n\n\nLet’s now update your prior with your classmate to your left’s score. Your n column should now be 30 to reflect the two quizzes.\nLet’s now update your prior with your classmate to your right’s score. Your n column should now be 45 to reflect the three quizzes.\n\n\nFinally, we will compare to the full class data. This will require tallying all the scores from all students."
  },
  {
    "objectID": "bayesian-activity/cnn-onion-beta-binomial.html#calculating-the-summary-statistics-and-plotting-the-distribution",
    "href": "bayesian-activity/cnn-onion-beta-binomial.html#calculating-the-summary-statistics-and-plotting-the-distribution",
    "title": "CNN vs. The Onion - Beta Binomial",
    "section": "Calculating the summary statistics and plotting the distribution",
    "text": "Calculating the summary statistics and plotting the distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteThe Optimist\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteUndecided\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteThe Pessimist\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteVery Optimistic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteYour prior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneReflection\n\n\n\n\nHow does observing more data affect the shape of the posterior?\nWhat happens to the posterior mean as you observe more correct or incorrect outcomes?\nIn what ways does the posterior reflect a compromise between prior belief and observed data?\n\n\n\n\nCompare to your class or to our dataset\n\n\n  student question correct year institution\n1       1        1       1 2010       Colby\n2       1        2       1 2010       Colby\n3       1        3       1 2010       Colby\n4       1        4       0 2010       Colby\n5       1        5       1 2010       Colby\n6       1        6       1 2010       Colby\n\n\nLet’s take a look at the student results from 2010.\n\n\n\n\n\n\n\n\n\n\nThe Optimist\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUndecided\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Pessimist\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVery Optimistic"
  },
  {
    "objectID": "bayesian-activity/cnn-onion-beta-binomial.html#comparison-of-the-priors",
    "href": "bayesian-activity/cnn-onion-beta-binomial.html#comparison-of-the-priors",
    "title": "CNN vs. The Onion - Beta Binomial",
    "section": "Comparison of the priors",
    "text": "Comparison of the priors\n\n\n\n\n\n\n\n\nFill in the gaps to add your alpha and beta shape parameters with your guess:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipReflection\n\n\n\n\nInspecting the updated posteriors, how did the posteriors change or differ based on the respective priors?\nWhat happened when you had a more informative prior, e.g. The Very Optimistic?\nLook back at the summary statistics that you calculated for the distributions. Choose one of the posteriors “Optimistic, Pessimistic, Undecided, Very Optimistic” and compare it with your own. What do each of the summary statistics (mean, mode, standard deviation) tell you about the probability of success after seeing the data?\n\n\n\n\n\n\n\n\n\nNoteRecap\n\n\n\nIn this activity, you:\n\nMade a prediction about how well you’d do on a quiz and turned that into a prior model\nLearned about different types of priors: optimistic, undecided, and pessimistic\nUpdated your prior using data from the quiz to get a posterior model\nCompared how different priors affect the posterior, even when we see the same data\nPracticed reading and interpreting plots of priors and posteriors\nCalculated key summary statistics like the mean, mode, and standard deviation\n\n\n\nThis exercise shows how Bayesian thinking helps us combine what we already believe with new evidence to make better, more informed decisions. And sometimes, it reminds us that even when we think we’re great at telling real news from fake news, the data might say otherwise!\n\n\n\n\n\n\nTipReflection\n\n\n\nThink about your prior prediction and how it compared to the data.\n\nHow similar or different was your prior to the actual results?\nDid updating your beliefs with data change your thinking? In what way?\nIf you had chosen a different prior (e.g., more vague or more informative), how would your posterior have changed?\nWhat does this activity show about the role of prior knowledge or assumptions in data analysis?\n\nTake a few minutes to jot down your thoughts before sharing your ideas with a partner or group."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CNN vs. Onion",
    "section": "",
    "text": "This repository contains two complementary classroom activities designed to introduce students to Bayesian and Frequentist approaches to reasoning under uncertainty. Each activity includes detailed facilitator guides, student materials, and R-based exercises for data exploration and model interpretation.\nEach activity is designed for classroom use with built-in flexibility for guided instruction or independent exploration. Facilitator guides provide setup instructions, discussion prompts, and example code."
  },
  {
    "objectID": "index.html#bayesian-activity-cnn-vs.-the-onion-modeling-prior-and-posterior-beliefs",
    "href": "index.html#bayesian-activity-cnn-vs.-the-onion-modeling-prior-and-posterior-beliefs",
    "title": "CNN vs. Onion",
    "section": "Bayesian Activity: CNN vs. The Onion – Modeling Prior and Posterior Beliefs",
    "text": "Bayesian Activity: CNN vs. The Onion – Modeling Prior and Posterior Beliefs\nThe goal of this activity is to explore how prior beliefs, what we think is probable before seeing any data, can influence the conclusions we draw after seeing new evidence (our posterior beliefs).\nStudents take a short quiz distinguishing real headlines (CNN) from satirical ones (The Onion), construct a prior model based on their expectations, and update it using quiz data to form a posterior model.\n\nLearning Objectives:\n\nConstruct and interpret prior and posterior models\nVisualize model updates with new data\nCalculate and compare summary statistics (mean, mode, standard deviation)\n\n\n\nActivity Resources\n\nClassroom Activity\nFacilitator Guide\nStudent Handout"
  },
  {
    "objectID": "index.html#frequentist-activity-hypothesis-testing-with-cnn-vs.-the-onion-quiz-data",
    "href": "index.html#frequentist-activity-hypothesis-testing-with-cnn-vs.-the-onion-quiz-data",
    "title": "CNN vs. Onion",
    "section": "Frequentist Activity: Hypothesis Testing with “CNN vs. The Onion” Quiz Data",
    "text": "Frequentist Activity: Hypothesis Testing with “CNN vs. The Onion” Quiz Data\nIn this activity, students use their quiz results to practice Frequentist reasoning. They simulate data, review distributions, and conduct both informal and formal hypothesis tests to assess their ability to distinguish real versus fake news headlines.\n\nLearning Objectives:\n\nReview probability distributions and sampling variability\nUse R to simulate data and compute test statistics\nPerform informal and formal hypothesis testing\n\n\n\nActivity Resources\n\nStudent Handout"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "CNN vs. Onion",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis work was supported by the National Science Foundation under Grant Nos #2215879, #2215920, and #2215709"
  },
  {
    "objectID": "bayesian-activity/cnn-onion-facilitator-guide.html",
    "href": "bayesian-activity/cnn-onion-facilitator-guide.html",
    "title": "CNN vs. The Onion Facilitator Guide",
    "section": "",
    "text": "This guide accompanies the CNN vs. The Onion - Beta Binomial activity, designed to help students explore the Bayesian updating process using real vs. fake news classification data. Students should enter the activity with a basic understanding of probability. This lesson will introduce how to construct and update Beta distributions as models for belief about a proportion.\n\n\n\n\n\n\nImportantPre-Activity Checklist\n\n\n\n\nPrint out one copy of the CNN Onion Student Handout per student.\nTest the google form quiz link in the CNN vs. The Onion - Beta Binomial activity."
  },
  {
    "objectID": "bayesian-activity/cnn-onion-facilitator-guide.html#overview",
    "href": "bayesian-activity/cnn-onion-facilitator-guide.html#overview",
    "title": "CNN vs. The Onion Facilitator Guide",
    "section": "",
    "text": "This guide accompanies the CNN vs. The Onion - Beta Binomial activity, designed to help students explore the Bayesian updating process using real vs. fake news classification data. Students should enter the activity with a basic understanding of probability. This lesson will introduce how to construct and update Beta distributions as models for belief about a proportion.\n\n\n\n\n\n\nImportantPre-Activity Checklist\n\n\n\n\nPrint out one copy of the CNN Onion Student Handout per student.\nTest the google form quiz link in the CNN vs. The Onion - Beta Binomial activity."
  },
  {
    "objectID": "bayesian-activity/cnn-onion-facilitator-guide.html#student-background-and-course-placement",
    "href": "bayesian-activity/cnn-onion-facilitator-guide.html#student-background-and-course-placement",
    "title": "CNN vs. The Onion Facilitator Guide",
    "section": "Student Background and Course Placement",
    "text": "Student Background and Course Placement\nThis activity is designed for students who have a foundational understanding of:\n\nBasic probability concepts (e.g., probability of success/failure) in a Binomial distribution\nWhat a probability distribution represents\nFamiliarity with the concept of conditional probability\n\nStudents do not need prior exposure to Bayesian statistics, but they should be comfortable with interpreting plots and understanding how beliefs or estimates can change with new information.\nWe expect that this activity would fit well in an Introduction to Probability and Statistics course particularly ones that includes elements of:\n\nStatistical Inference\nBayesian concepts\nApplied reasoning with real-world data\n\nIt could be introduced after students have learned about conditional probability and Bayes’ Rule.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\nBy the end of this activity, students will be able to:\n\nDefine and distinguish between prior and posterior distributions.\nUse the Beta distribution to construct prior beliefs about a probability.\nApply Bayesian updating to revise beliefs with observed data.\nInterpret summary statistics (mean, mode, standard deviation) of Beta distributions.\nReflect on how prior information and data interact to shape posterior conclusions."
  },
  {
    "objectID": "bayesian-activity/cnn-onion-facilitator-guide.html#activity-sequence",
    "href": "bayesian-activity/cnn-onion-facilitator-guide.html#activity-sequence",
    "title": "CNN vs. The Onion Facilitator Guide",
    "section": "Activity Sequence",
    "text": "Activity Sequence\n\n1. Introduce the Context\n\nPresent the quiz task: students will identify whether a headline came from CNN or The Onion.\nExplain that students will make a prior prediction, then update it using quiz results.\n\n\n\n2. Construct Priors\n\nStudents choose a prior model for the proportion of correct answers they expect people to get on the quiz and translate it into a Beta distribution.\n\nOptimistic: Beta(8, 3)\nUndecided: Beta(1, 1)\nPessimistic: Beta(5, 10)\nVery Optimistic: Beta(140, 10)\n\n\n\n\n3. Visualize Priors\n\nUse plot_beta() to examine shapes of priors.\nStudents compare their prior to reference priors and discuss informativeness.\n\n\n\n4. Take the Quiz\n\nStudents complete the quiz and count their correct responses.\n\n\n\n5. Update with Data\n\nStudents update their prior using their own score: plot_beta_binomial() and summarize_beta_binomial().\nStudents repeat the update process adding classmates’ scores (n = 30, then n = 45) tallying the correct responses each time.\n\n\n\n6. Update with the Class Data or the Provided Data\n\nUse the provided data or ask each student to share their score. Add these together. n will become 15 x number of students.\nStudents update their prior using the class score: plot_beta_binomial() and summarize_beta_binomial().\n\n\n\n7. Compare Posteriors\n\nVisual comparison: look at posterior distributions from different priors side by side.\nNumeric comparison: interpret changes in mean, mode, and standard deviation."
  },
  {
    "objectID": "bayesian-activity/cnn-onion-facilitator-guide.html#reflection-guidance",
    "href": "bayesian-activity/cnn-onion-facilitator-guide.html#reflection-guidance",
    "title": "CNN vs. The Onion Facilitator Guide",
    "section": "Reflection Guidance",
    "text": "Reflection Guidance\n\n\n\n\n\n\nNotePrior Informativeness\n\n\n\n\nPrompt: Is your prior informative or vague?\nExample Responses:\n\n“My prior was vague because I used Beta(1,1).”\n“Beta(10, 3) was informative since I was confident in my guess.”\n\n\n\n\n\n\n\n\n\n\nNotePosterior Updates\n\n\n\n\nPrompt: How did the posterior change after seeing data?\nExample Responses:\n\n“My posterior mean increased because people did better than expected on the quiz.”\n“Even though my prior was optimistic, the data pulled the posterior lower because people did not as well as expected.”\n\n\n\n\n\n\n\n\n\n\nNoteAdding More Data\n\n\n\n\nPrompt: What happens as you add more data?\nExample Responses:\n\n“The distribution became more concentrated.”\n“The standard deviation decreased.”\n“The posterior gets closer to the proportion (number correct/total questions) from the data”\n\n\n\n\n\n\n\n\n\n\nNoteComparing Prior Models\n\n\n\n\nPrompt: How did different priors respond to the same data?\nExample Responses:\n\n“The optimistic prior resulted in a higher posterior mean than the undecided one.”\n“The undecided prior was the most influenced by the data.”\n\n\n\n\n\n\n\n\n\n\nNoteBig Picture\n\n\n\n\nPrompt: What does this activity show about the role of prior models?\nExample Responses:\n\n“Priors matter when the data is limited.”\n“The more data we have, the more it outweighs our prior.”\n\n\n\n\n\n\n\n\n\n\nNoteFurther Discussion Questions\n\n\n\n\nWhat would a prior look like that represents being totally uncertain?\nIn what scenarios might you want an informative prior?\nIf two people see the same data but have different priors, should they agree?\n\n\n\n\n\n\n\n\n\nTipTips\n\n\n\n\nCheck that students understand: the Beta distribution is a model for belief about a probability.\nReinforce that the prior is not “wrong” even if it differs from data, it reflects what we believed before seeing the data.\nEncourage collaborative discussion when comparing prior/posterior plots."
  },
  {
    "objectID": "bayesian-activity/cnn-onion-facilitator-guide.html#common-questions-that-might-arise",
    "href": "bayesian-activity/cnn-onion-facilitator-guide.html#common-questions-that-might-arise",
    "title": "CNN vs. The Onion Facilitator Guide",
    "section": "Common questions that might arise:",
    "text": "Common questions that might arise:\n\n\n\n\n\n\nTipWhy is the Uniformed Distribution Beta(1, 1)\n\n\n\n\nBeta(1,1) is the uniform distribution over the interval [0,1]. This means that before seeing any data, we assign equal plausibility to all possible values of \\(\\pi\\), the probability that someone guesses a headline correctly.\nWe’re saying we have no reason to favor any particular value of \\(\\pi\\) between 0 and 1. In contrast, Beta(8,8) assumes some structure, it implies we already believe the success rate is centered around 0.5 with some confidence.\n\n\n\n\n\n\n\n\n\nTipHow to interpret a Density Plot\n\n\n\n\nStart by looking at the x-axis, which shows the possible values of the parameter (here, the probability \\(\\pi\\) of guessing correctly), and the y-axis, which shows the plausibility or density of those values.\nThe peak of the curve indicates the most likely value (the mode), and the width reflects uncertainty—narrow curves suggest high certainty, while wide curves indicate more uncertainty.\nThe area under the curve between two points represents how probable it is that the parameter falls in that range.\nWhen comparing prior and posterior distributions, look at how the curve shifts and narrows to see how beliefs change with data.\nAlways describe what you see: Is the distribution centered high or low? Is it narrow or wide? Did the data change the belief significantly?"
  },
  {
    "objectID": "bayesian-activity/cnn-onion-facilitator-guide.html#references-and-acknowledgements",
    "href": "bayesian-activity/cnn-onion-facilitator-guide.html#references-and-acknowledgements",
    "title": "CNN vs. The Onion Facilitator Guide",
    "section": "References and Acknowledgements",
    "text": "References and Acknowledgements\n\nThis work was supported by the National Science Foundation under Grant Nos #2215879, #2215920, and #2215709.\nJohnson, A.A., Ott, M.Q., & Doğucu, M. (2022). Bayes Rules!: An Introduction to Applied Bayesian Modeling (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9780429288340. 📘\nDoğucu M, Johnson A, Ott M (2021). bayesrules: Datasets and Supplemental Functions from Bayes Rules! Book. R package version 0.0.2.9000. 🔗."
  },
  {
    "objectID": "bayesian-activity/cnn-onion-student-handout.html",
    "href": "bayesian-activity/cnn-onion-student-handout.html",
    "title": "CNN Onion Handout",
    "section": "",
    "text": "What was your initial prior belief about the proportion of quiz answers a person would get right?\n\n\nWhich predefined prior (Optimistic, Undecided, Pessimistic) did your belief most closely align with?\n\n\nWould you describe your prior as informative or vague? Why?\n\n\n\n\n\nLook at the plot of your prior distribution. What does it tell you about your expectations?\n\n\nHow would a more informative prior (e.g., Beta(140, 10)) affect the shape of the distribution compared to a vague one?\n\n\n\n\nFill out the Quiz Results table after taking the quiz\n\n\n\nParticipant\nCorrect\nIncorrect\n\n\n\n\nYour Score\n_______\n_________\n\n\nPerson on Your Left\n_______\n_________\n\n\nPerson on Your Right\n_______\n_________\n\n\nClass Total\n_______\n_________\n\n\n\n\nHow did your posterior distribution change after observing your own quiz results?\n\n\nHow did your posterior change when you added a classmate’s score? What about after pooling the full class data?\n\n\nWhat do you notice about how increasing the amount of data affects the posterior distribution?\n\n\n\n\n\nCompare your posterior with one of the predefined priors (Optimistic, Pessimistic, etc.). What are the differences in mean, mode, and standard deviation?\n\n\nWhat do these summary statistics tell you about your belief in someone guessing correctly after seeing the data?\n\n\n\n\n\nInspecting the updated posteriors, how did the posteriors change or differ based on the respective priors?\n\n\nWhat happened when you had a more informative prior, e.g. The Very Optimistic?\n\n\nLook back at the summary statistics that you calculated for the distributions. Choose one of the posteriors “Optimistic, Pessimistic, Undecided, Very Optimistic” and compare it with your own. What do each of the summary statistics (mean, mode, standard deviation) tell you about the probability of success after seeing the data?\n\n\nThink about your prior prediction and how it compared to the data.\n\n\nHow similar or different was your prior to the actual results?\n\n\nDid updating your beliefs with data change your thinking? In what way?\n\n\nIf you had chosen a different prior (e.g., more vague or more informative), how would your posterior have changed?\n\n\nWhat does this activity show about the role of prior knowledge or assumptions in data analysis?\n\nTake a few minutes to jot down your thoughts before share your ideas with a partner or group."
  },
  {
    "objectID": "bayesian-activity/cnn-onion-student-handout.html#discussion-questions",
    "href": "bayesian-activity/cnn-onion-student-handout.html#discussion-questions",
    "title": "CNN Onion Handout",
    "section": "",
    "text": "What was your initial prior belief about the proportion of quiz answers a person would get right?\n\n\nWhich predefined prior (Optimistic, Undecided, Pessimistic) did your belief most closely align with?\n\n\nWould you describe your prior as informative or vague? Why?\n\n\n\n\n\nLook at the plot of your prior distribution. What does it tell you about your expectations?\n\n\nHow would a more informative prior (e.g., Beta(140, 10)) affect the shape of the distribution compared to a vague one?\n\n\n\n\nFill out the Quiz Results table after taking the quiz\n\n\n\nParticipant\nCorrect\nIncorrect\n\n\n\n\nYour Score\n_______\n_________\n\n\nPerson on Your Left\n_______\n_________\n\n\nPerson on Your Right\n_______\n_________\n\n\nClass Total\n_______\n_________\n\n\n\n\nHow did your posterior distribution change after observing your own quiz results?\n\n\nHow did your posterior change when you added a classmate’s score? What about after pooling the full class data?\n\n\nWhat do you notice about how increasing the amount of data affects the posterior distribution?\n\n\n\n\n\nCompare your posterior with one of the predefined priors (Optimistic, Pessimistic, etc.). What are the differences in mean, mode, and standard deviation?\n\n\nWhat do these summary statistics tell you about your belief in someone guessing correctly after seeing the data?\n\n\n\n\n\nInspecting the updated posteriors, how did the posteriors change or differ based on the respective priors?\n\n\nWhat happened when you had a more informative prior, e.g. The Very Optimistic?\n\n\nLook back at the summary statistics that you calculated for the distributions. Choose one of the posteriors “Optimistic, Pessimistic, Undecided, Very Optimistic” and compare it with your own. What do each of the summary statistics (mean, mode, standard deviation) tell you about the probability of success after seeing the data?\n\n\nThink about your prior prediction and how it compared to the data.\n\n\nHow similar or different was your prior to the actual results?\n\n\nDid updating your beliefs with data change your thinking? In what way?\n\n\nIf you had chosen a different prior (e.g., more vague or more informative), how would your posterior have changed?\n\n\nWhat does this activity show about the role of prior knowledge or assumptions in data analysis?\n\nTake a few minutes to jot down your thoughts before share your ideas with a partner or group."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "frequentist-activity/cnn-onion-template.html",
    "href": "frequentist-activity/cnn-onion-template.html",
    "title": "CNN vs. The Onion",
    "section": "",
    "text": "TipLearning objectives\n\n\n\nBy the end of the activity you’ll be able to:\n\n\nReview distributions\n\n\n\nUse basic R commands to simulate data\n\n\n\nConduct an informal hypothesis test\n\n\n\nConduct a formal hypothesis test\n\n\n\n\nCNN (the Cable News Network) is widely considered a reputable news source. The Onion, on the other hand, is (according to Wikipedia) “an American news satire organization. It is an entertainment newspaper and a website featuring satirical articles reporting on international, national, and local news.” Another way of putting it - The Onion is “fake news” for entertainment purposes.\nIn this lab you will assess your ability to determine real news stories published on cnn.com from fake news stories published on theonion.com.\nEach of you will take a quiz consisting of 15 questions. Each question has the same possible answers: CNN or The Onion.\nLet \\(\\hat{p}\\) = the proportion of questions you answer correctly.\nQuestion 1: Make a guess. What do you think \\(\\hat{p}\\) will be for you (i.e. before looking at the quiz, what proportion of questions do you think you’ll answer correctly)?\nQuestion 2: How do you think p-hat is distributed? In other words, what values can \\(\\hat{p}\\) be? Are they all equally likely? What shape will it have? On a sheet of paper, draw a picture of what you think the distribution of \\(\\hat{p}\\) will look like.\nQuestion 3: Take the Quiz. What proportion did you answer correctly?\nQuestion 4: Do you think your strategy for choosing the correct answer is better than just guessing (e.g. choosing a response randomly)? Suppose that instead of thinking about each question and answering to the best of your ability, you randomly guessed answers (e.g. you flipped a coin – heads = CNN, tails = The Onion). Under this scenario, what would you expect \\(\\hat{p}\\) to be? Discuss your answers with a neighbor.\nQuestion 5: Suppose we kept taking similar quizzes, but each time, we employed the random guessing strategy. What would the distribution of \\(\\hat{p}\\) look like? What values of \\(\\hat{p}\\) would be typical? What values would be unusual?\nQuestion 6: Can you think of a way to simulate such a distribution? Discuss with a neighbor.\nIt turns out we can use a computer to easily simulate what the distribution of p-hat would look like in such a scenario using the following code. Below we’ll simulate the distribution of \\(\\hat{p}\\) for 1000 students taking a 15 question quiz using the guessing strategy (probability of getting a question correct is 0.5).\nQuestion 7:. Run the simulation and examine the shape. How would you describe it? Is it similar to what you predicted?\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8:. Now consider your value of \\(\\hat{p}\\) (from question 3) and compare it to the simulated distribution. Where does it fall? Is it close to the middle or is it out towards an extremity of the distribution?\nIf your value of \\(\\hat{p}\\) falls close to the center of values from the simulated distribution, then your method of guessing is consistent with the random guessing strategy (i.e. your method of choosing answers is no better than just guessing). However, if your value of \\(\\hat{p}\\) is closer to one of the tails of the simulated distribution, then your method of guessing is not consistent with the random guessing strategy (i.e. your method of choosing the answer is better (or worse) than just guessing.\nYou’ve just conducted an informal hypothesis test!"
  },
  {
    "objectID": "frequentist-activity/cnn-onion-template.html#activity-introduction",
    "href": "frequentist-activity/cnn-onion-template.html#activity-introduction",
    "title": "CNN vs. The Onion",
    "section": "",
    "text": "TipLearning objectives\n\n\n\nBy the end of the activity you’ll be able to:\n\n\nReview distributions\n\n\n\nUse basic R commands to simulate data\n\n\n\nConduct an informal hypothesis test\n\n\n\nConduct a formal hypothesis test\n\n\n\n\nCNN (the Cable News Network) is widely considered a reputable news source. The Onion, on the other hand, is (according to Wikipedia) “an American news satire organization. It is an entertainment newspaper and a website featuring satirical articles reporting on international, national, and local news.” Another way of putting it - The Onion is “fake news” for entertainment purposes.\nIn this lab you will assess your ability to determine real news stories published on cnn.com from fake news stories published on theonion.com.\nEach of you will take a quiz consisting of 15 questions. Each question has the same possible answers: CNN or The Onion.\nLet \\(\\hat{p}\\) = the proportion of questions you answer correctly.\nQuestion 1: Make a guess. What do you think \\(\\hat{p}\\) will be for you (i.e. before looking at the quiz, what proportion of questions do you think you’ll answer correctly)?\nQuestion 2: How do you think p-hat is distributed? In other words, what values can \\(\\hat{p}\\) be? Are they all equally likely? What shape will it have? On a sheet of paper, draw a picture of what you think the distribution of \\(\\hat{p}\\) will look like.\nQuestion 3: Take the Quiz. What proportion did you answer correctly?\nQuestion 4: Do you think your strategy for choosing the correct answer is better than just guessing (e.g. choosing a response randomly)? Suppose that instead of thinking about each question and answering to the best of your ability, you randomly guessed answers (e.g. you flipped a coin – heads = CNN, tails = The Onion). Under this scenario, what would you expect \\(\\hat{p}\\) to be? Discuss your answers with a neighbor.\nQuestion 5: Suppose we kept taking similar quizzes, but each time, we employed the random guessing strategy. What would the distribution of \\(\\hat{p}\\) look like? What values of \\(\\hat{p}\\) would be typical? What values would be unusual?\nQuestion 6: Can you think of a way to simulate such a distribution? Discuss with a neighbor.\nIt turns out we can use a computer to easily simulate what the distribution of p-hat would look like in such a scenario using the following code. Below we’ll simulate the distribution of \\(\\hat{p}\\) for 1000 students taking a 15 question quiz using the guessing strategy (probability of getting a question correct is 0.5).\nQuestion 7:. Run the simulation and examine the shape. How would you describe it? Is it similar to what you predicted?\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8:. Now consider your value of \\(\\hat{p}\\) (from question 3) and compare it to the simulated distribution. Where does it fall? Is it close to the middle or is it out towards an extremity of the distribution?\nIf your value of \\(\\hat{p}\\) falls close to the center of values from the simulated distribution, then your method of guessing is consistent with the random guessing strategy (i.e. your method of choosing answers is no better than just guessing). However, if your value of \\(\\hat{p}\\) is closer to one of the tails of the simulated distribution, then your method of guessing is not consistent with the random guessing strategy (i.e. your method of choosing the answer is better (or worse) than just guessing.\nYou’ve just conducted an informal hypothesis test!"
  },
  {
    "objectID": "frequentist-activity/cnn-onion-template.html#explanation",
    "href": "frequentist-activity/cnn-onion-template.html#explanation",
    "title": "CNN vs. The Onion",
    "section": "Explanation:",
    "text": "Explanation:\nIn statistics, hypothesis tests allow us to test whether a particular value for a parameter is plausible based on our sample data. We hypothesize a specific value for the parameter of interest, and then determine the probability of observing data as or more extreme as the data that were actually obtained assuming our hypothesized value is correct. If this probability is low, it means our observed data are unlikely using the assumed parameter value. In other words, the assumed parameter value is not plausible. On the other hand, if the probability is high, it means our observed data are typical using the assumed parameter value – it is plausible.\nThe probability described above has come to be known as a p-value. A p-value is a conditional probability. It tells us the probability of observing the data as or more extreme as the data that were actually obtained conditional on our hypothesized parameter value being true.\nThere are a few common steps that you’ll need to complete each time you conduct a hypothesis test:\n\nState the parameter of interest\n\nState a null and an alternative hypothesis\n\nCheck conditions for the test\n\nDetermine the p-value\n\nSummarize the results of the test without using statistical jargon\n\nParameter\nLet’s do an example using the CNN/Onion data. First let’s define our population and parameter of interest.\npopulation = all students at this school/university that have taken or our taking this course\np = underlying probability of correctly answering a question on the CNN/ONION quiz\nHypotheses\nIf we were just guessing at the answers and not using any strategy, we’d expect to get about half of the questions right. So p = 0.50. Is this a plausible value for p? Or might it be higher since students presumably are not just guessing? Suggest an appropriate null and an alternative hypothesis for this test. (Hint: when testing a proportion, the null hypothesis should always include the parameter of interest and specify the assumed value for the parameter).\nHo:\np = 0.50\nHa:\np &gt; 0.50\nConditions\nThe conditions required for hypothesis testing to be valid vary based on the parameter(s) of interest and the test being conducted. When testing a single proportion, the sample needs to be “large”. In our example the sample refers to the number of questions on the quiz. How large the sample needs to be depends on what the underlying value of p is. If p is close to 0.5, the sample size doesn’t need to be as large. However, if p is closer to 0 or 1.0, then the sample size needs to be much larger. Where the data come from also matters. Is it a representative sample from the population of interest (great!), or not? More generally, these types of hypothesis tests will work well when the randomization distribution is approximately bell-shaped and symmetric (see below).\nP-value\nOne way to obtain the p-value for this test is to simulate what the distribution of our sample statistic would look like if the null hypothesis were true (known as a randomization test; the simulated distribution is known as a randomization distribution). Once we have the randomization distribution, we use it to find the probability of getting a sample statistic at least as extreme as the one that was actually observed. This value is the p-value of the test. For example, suppose I took the quiz and got 9/15 headlines correct, using:\n:::\n\n\n\n\n\n\n\n\nI find the area greater than or equal to 9/15 in a randomization distribution to be 0.298 (you should get the same value if you used set.seed(123))\nSummary\nSince this probability isn’t very small I would conclude that my data are consistent with the null hypothesis, meaning my strategy of choosing the right answer is consistent with the guessing strategy (i.e. my strategy is no better than what I’d expect to get by just guessing).\nThere is also a way to conduct this test using the binom.test command in R. This test relies on theoretical probabilities derived from the binomial distribution. The command:\nbinom.test(9,15,0.5, alternative = “greater”) should produce analogous results to the test described above."
  }
]